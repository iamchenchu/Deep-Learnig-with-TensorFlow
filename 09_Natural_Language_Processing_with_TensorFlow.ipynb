{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrV+OlYM/9K0PUQExkt021",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamchenchu/Deep-Learnig-with-TensorFlow/blob/main/09_Natural_Language_Processing_with_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP(Naturala Language Processing) and NLU (Natural Language Understanding) :\n",
        "\n",
        "`A handful of example natural language processing (NLP) and natural language understanding (NLU) problems. These are also often referred to as sequence problems (going from one sequence to another)`\n",
        "\n",
        "**Let's see where do we use it : Which are reffered to as a Sequence Problems**\n",
        "\n",
        "1. MultiLabel Classification\n",
        "2. Machine Translation\n",
        "3. Text Generation\n",
        "4. Voice Assistant\n",
        "\n",
        "**Types of Sequence problems :(input to output types)**\n",
        "1. One to one  (one word translation)\n",
        "2. One to many (Image captioning, possible to have multiple captions)\n",
        "3. Many to one (Sentiment analysis maybe a youtube comment, BitCoin Price Prediction)\n",
        "4. many to many (Part-of-speech tagging in natural language processing. Each word in the input sentence may correspond to multiple possible part-of-speech tags.)\n",
        "5. many to many (DNA sequence alignment. Aligning two DNA sequences where each base in one sequence may correspond to multiple bases in the other.)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rmxLc6AgR2bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What will we do in this section :         \n",
        "\n",
        "* Downloading and preparing the text data\n",
        "* How to prepare text data for modeling (Tokenization and embedding)\n",
        "* Setting up multiple modeling experiments with Recurrent Neural Networks (RNN)\n",
        "* Building a text feature extraction model using TensorFlow Hub\n",
        "* Finding the most wrong prediction examples\n",
        "* Using a model we've built to make predictions on the text from the wild\n",
        "\n",
        "\n",
        "\n",
        "**NLP Inputs and outputs :**\n",
        "\n",
        "`Text -> turn into numbers -> build a model -> train the model to find patterns -> use patterns (make predictions)`\n",
        "\n",
        "esource: For a great overview of NLP and the different problems within it, read the article A Simple Introduction to [Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32).\n",
        "\n"
      ],
      "metadata": {
        "id": "la_EqoazVvw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEPS  WE FOLLOW IN CREATING IN RNN / CNN FOR NLP\n",
        "\n",
        "1. Get the data ready (Turn all data into numbers as neural networks can't handle text or natural language and make sure all of your tensors are in right shape pad sequence which don't fit )\n",
        "2. Build or pick a pretrained model (to suit your problem)\n",
        "3. Fit the model to the data and make a prediction\n",
        "4. Evaluate the model\n",
        "5. Improve through Experimentation\n",
        "6. Save and reload your trained model\n",
        "\n"
      ],
      "metadata": {
        "id": "lcNQjqG-Whiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TYPICAL ARCHITECTURE OF A RECCURENT NEURAL NETWORK\n",
        "\n",
        "**What is RNN :**  is class of Artificial Neural Networks where connections between nodes from a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behaviour.\n",
        "\n",
        "1. Input Text\n",
        "2. Input Layers\n",
        "3. Text vectorization layer\n",
        "4. Embedding (Turns mapping of text vectors to embedding matrix (representing how words relate Ex King-man+woman = queen))\n",
        "5. RNN Cells (Finds Pattern in sequence) `Ex : Simple RNN, LSTM, GRU`\n",
        "6. Hidden Activations : Adds non linearity to learned features usually use `tanh`\n",
        "7. Pooling layer : Reduces the dimensionality of the learned features usually Conv1D models `Ex : GlobalAveragePooling1D or GlobalMaxPool1D`\n",
        "8. Fully Connected layer : Further refines learned features from recurrent layers (Dense layer)\n",
        "9. Output layer (Takes learned feature and outputs them in shape of target labels) Ex : `output_shape = [number_of_classes] (Disaster or not)`\n",
        "10. Output Activation : Adds non-linearities to output layer.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "31MGB-JyZqZG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0TmZXlahCtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}